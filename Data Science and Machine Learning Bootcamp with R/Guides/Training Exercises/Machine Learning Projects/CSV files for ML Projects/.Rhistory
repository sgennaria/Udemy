head(temp.results)
ggplot(temp.results, aes(x = temp, y = predicted)) + geom_point() + theme_minimal()
temp.results
sort(temp.results, temp)
sort(temp.results, temp.results$temp)
sort(temp.results$temp)
temp.results[temp.results$temp==25, ]
temp.results[round(temp.results$temp)==25, ]
mean(temp.results[round(temp.results$temp)==25, ])
mean(temp.results$predicted[round(temp.results$temp)==25, ])
mean(temp.results[round(temp.results$temp)==25, ]$predicted)
str(bike)
bike$hour <- as.numeric(bike$hour)
str(bike)
library(corrgram)
corrgram(bike, order=TRUE, lower.panel=panel.shade, upper.panel=panel.pie, text.panel=panel.txt)
complex.model <- lm(count ~ season + holiday + workingday + weather + temp + humidity + windspeed + factor(hour), bike)
summary(complex.model)
complex.model <- lm(count ~ season + holiday + workingday + weather + temp + humidity + windspeed + hour, bike)
summary(complex.model)
ggplot(bike, aes(temp, count)) + geom_point(aes(color=temp), alpha = 0.2) + theme_minimal()
ggplot(bike, aes(datetime, count)) + geom_point(aes(color=temp), alpha = 0.5) + scale_x_datetime() + scale_color_gradient(high='orangered', low='cyan') + theme_minimal()
cor(bike[,c('temp', 'count')])
ggplot(bike, aes(factor(season), count)) + geom_boxplot(aes(color = factor(season))) + theme_minimal()
library(dplyer)
library(dplyr)
library(dplyr)
pl <- ggplot(filter(bike, workingday==1), aes(hour, count))
pl <- pl + geom_point(position=position_jitter(w=1, h=0), aes(color=temp), alpha=0.5)
pl <- pl + scale_color_gradientn(colors=c('dark blue', 'blue', 'light blue', 'light green', 'yellow', 'orange', 'red'))
pl + theme_minimal()
pl <- ggplot(filter(bike, workingday==0), aes(hour, count))
pl <- pl + geom_point(position=position_jitter(w=1, h=0), aes(color=temp), alpha=0.5)
pl <- pl + scale_color_gradientn(colors=c('dark blue', 'blue', 'light blue', 'light green', 'yellow', 'orange', 'red'))
pl + theme_minimal()
temp.model <- lm(count ~ temp, bike)
summary(temp.model)
6.0462 + (9.1705 * 25)
temp.test <- data.frame(temp=c(25))
str(temp.test)
temp.test <- data.frame(temp=25)
str(temp.test)
predict(temp.model,temp.test)
temp.predictions <- predict(temp.model, bike)
temp.results <- cbind(temp.predictions, bike$temp)
colnames(temp.results) <- c('predicted', 'temp')
temp.results <- as.data.frame(temp.results)
mean(temp.results[round(temp.results$temp)==25, ]$predicted)
complex.model <- lm(count ~ . -casual -registered -datetime -atemp, bike)
summary(complex.model)
install.packages('rpart')
library(rpart)
str(kyphosis)
head(kyphosis)
tree <- rpart(Kyphosis ~ ., method='class', data=kyphosis)
printcp(tree)
plot(tree, uniform=TRUE, main='Kyphosis Tree')
text(tree, use.n=TRUE, all=TRUE)
install.packages('rpart.plot')
library(rpart.plot)
prp(tree)
install.packages('randomForest')
library(randomForest)
rf.model <- randomForest(Kyphosis ~ ., data=kyphosis)
print(rf.model)
rf.model$ntree
rf.model$confusion
library(ISLR)
str(College)
head(College)
data(College)
str(College)
head(College)
data()
library(ggplot2)
ggplot(College, aes(Room.Board, Grad.Rate)) + geom_point(aes(col=Private))
str(College)
ggplot(College, aes(F.Undergrad)) + geom_histogram(aes(col=Private))
ggplot(College, aes(F.Undergrad)) + geom_histogram(aes(fill=Private))
ggplot(College, aes(F.Undergrad)) + geom_histogram(aes(col='black', fill=Private))
ggplot(College, aes(F.Undergrad)) + geom_histogram(col='black', aes(fill=Private))
ggplot(College, aes(F.Undergrad)) + geom_histogram(col='black', aes(fill=Private). binwidth=1)
ggplot(College, aes(F.Undergrad)) + geom_histogram(col='black', aes(fill=Private), binwidth=1)
ggplot(College, aes(F.Undergrad)) + geom_histogram(col='black', aes(fill=Private), binwidth=16)
ggplot(College, aes(F.Undergrad)) + geom_histogram(col='black', aes(fill=Private), binwidth=150)
ggplot(College, aes(F.Undergrad)) + geom_histogram(col='black', aes(fill=Private), binwidth=1500)
ggplot(College, aes(F.Undergrad)) + geom_histogram(col='black', aes(fill=Private), binwidth=500)
ggplot(College, aes(F.Undergrad)) + geom_histogram(col='black', aes(fill=Private), binwidth=1000)
ggplot(College, aes(F.Undergrad)) + geom_histogram(col='black', aes(fill=Private), binwidth=750)
ggplot(College, aes(F.Undergrad)) + geom_histogram(col='black', aes(fill=Private), binwidth=600)
ggplot(College, aes(F.Undergrad)) + geom_histogram(col='black', aes(fill=Private), binwidth=650)
ggplot(College, aes(Grad.Rate)) + geom_histogram(col='black', aes(fill=Private), binwidth=650)
ggplot(College, aes(Grad.Rate)) + geom_histogram(col='black', aes(fill=Private), binwidth=2)
ggplot(College, aes(Grad.Rate)) + geom_histogram(col='black', aes(fill=Private), binwidth=3)
ggplot(College, aes(Grad.Rate)) + geom_histogram(col='black', aes(fill=Private), binwidth=2)
ggplot(College, aes(Grad.Rate)) + geom_histogram(col='black', aes(fill=Private), binwidth=2.5)
ggplot(College, aes(Grad.Rate)) + geom_histogram(col='black', aes(fill=Private), binwidth=2)
ggplot(College, aes(Grad.Rate)) + geom_histogram(col='black', aes(fill=Private), binwidth=2.5)
library(dplyr)
College %>% filter(Grad.Rate > 100)
College[Grad.Rate > 100,]
College['Grad.Rate' > 100,]
College[College$Grad.Rate > 100,]
College[College$Grad.Rate > 100,]$Grad.Rate
College[College$Grad.Rate > 100,]$Grad.Rate <- 100
College[College$Grad.Rate > 100,]
library(caTools)
str(College)
set.seed(101)
split <- sample.split(College$Private, SplitRatio = 0.7)
train <- subset(College, split==TRUE)
test <- subset(College, split==FALSE)
str(train)
str(test)
library(rpart)
tree <- rpart(Private ~ ., method='class', data=train)
fitted.probabilities <- predict(tree, test, type='response')
fitted.probabilities <- predict(tree, test)
fitted.probabilities <- predict(tree, test, type='response')
fitted.probabilities <- predict(tree, test)
fitted.results <- ifelse(fitted.probabilities>0.5, 1, 0)
misClassError <- mean(fitted.results != final.test$Private)
misClassError <- mean(fitted.results != test$Survived)
1-misClassError
head(fitted.probabilities)
fitted.results <- ifelse(fitted.probabilities>0.5, 1, 0)
str(College)
str(fitted.results)
head(fitted.results)
tree.preds <- predict(tree, test)
head(tree.preds)
pred.results <- ifelse(tree.preds>0.5, 1, 0)
str(pred.results)
head(pred.results)
tree.preds$Private <- ifelse(pred.results==1, 'Yes', 'No')
head(tree.preds)
tree.preds <- predict(tree, test)
head(tree.preds)
pred.results <- ifelse(tree.preds>0.5, 1, 0)
str(pred.results)
head(pred.results)
pred.results$Private <- ifelse(Yes==1, 'Yes', 'No')
pred.results$Private <- ifelse(pred.results$Yes==1, 'Yes', 'No')
typeof(pred.results)
typeof(tree.preds)
tree.preds <- as.data.frame(tree.preds)
tree.preds$Private <- ifelse(pred.results==1, 'Yes', 'No')
head(tree.preds)
tree.preds <- predict(tree, test)
tree.preds <- as.data.frame(tree.preds)
tree.preds$Private <- ifelse(pred.results$Yes==1, 'Yes', 'No')
pred.results <- as.data.frame(pred.results)
pred.results$Private <- ifelse(Yes==1, 'Yes', 'No')
pred.results$Private <- ifelse(pred.results$Yes==1, 'Yes', 'No')
head(pred.results)
head(tree.preds)
tree.preds$Private <- ifelse(pred.results$Yes==1, 'Yes', 'No')
head(tree.preds)
table(tree.preds$Private, test$Private)
library(rpart.plot)
prp(tree)
library(randomForest)
rf.model <- randomForest(Private ~ ., data=train, importance=TRUE)
rf.model$confusion
rf.model$importance
rf.preds <- predict(rf.model, test)
str(rf.preds)
head(rf.preds)
table(rf.preds$Private, test$Private)
rf.preds <- as.data.frame(rf.preds)
table(rf.preds$Private, test$Private)
rf.preds <- predict(rf.model, test)
rf.pred.results <- ifelse(rf.preds>0.5, 1, 0)
rf.preds <- predict(rf.model, test)
head(rf.preds)
table(rf.preds, test$Private)
table(tree.preds$Private, test$Private)
cm.report <- function(cm) {
accuracy <- (cm[1,1] + cm[2,2]) / sum(cm)
error.rate <-(cm[1,2] + cm[2,1]) / sum(cm)
recall <- cm[2,2] / sum(cm[2,])
precision <- cm[2,2] / sum(cm[,2])
paste0('Accuracy: ', accuracy)
paste0('Error Rate: ', error.rate)
paste0('Recall: ', recall)
paste0('Precision: ', precision)
}
cm.report(cm1)
cm1 <- table(rf.preds, test$Private)
cm2 <- table(tree.preds$Private, test$Private)
cm.report(cm1)
cm.report <- function(cm) {
accuracy <- (cm[1,1] + cm[2,2]) / sum(cm)
error.rate <-(cm[1,2] + cm[2,1]) / sum(cm)
recall <- cm[2,2] / sum(cm[2,])
precision <- cm[2,2] / sum(cm[,2])
cat(paste0('Accuracy: ', accuracy)
paste0('Error Rate: ', error.rate)
paste0('Recall: ', recall)
paste0('Precision: ', precision))
}
cm.report(cm1)
cm.report <- function(cm) {
accuracy <- (cm[1,1] + cm[2,2]) / sum(cm)
error.rate <-(cm[1,2] + cm[2,1]) / sum(cm)
recall <- cm[2,2] / sum(cm[2,])
precision <- cm[2,2] / sum(cm[,2])
paste0('Accuracy: ', accuracy, '
Error Rate: ', error.rate, '
Recall: ', recall, '
Precision: ', precision)
}
cm.report(cm1)
cm.report <- function(cm) {
accuracy <- (cm[1,1] + cm[2,2]) / sum(cm)
error.rate <-(cm[1,2] + cm[2,1]) / sum(cm)
recall <- cm[2,2] / sum(cm[2,])
precision <- cm[2,2] / sum(cm[,2])
cat('Accuracy: ', accuracy, '
Error Rate: ', error.rate, '
Recall: ', recall, '
Precision: ', precision)
}
cm.report(cm1)
cm.report <- function(cm) {
accuracy <- (cm[1,1] + cm[2,2]) / sum(cm)
error.rate <-(cm[1,2] + cm[2,1]) / sum(cm)
recall <- cm[2,2] / sum(cm[2,])
precision <- cm[2,2] / sum(cm[,2])
cat('Accuracy: ', accuracy)
cat('Error Rate: ', error.rate)
cat('Recall: ', recall)
cat('Precision: ', precision))
}
cm.report(cm1)
cm.report <- function(cm) {
accuracy <- (cm[1,1] + cm[2,2]) / sum(cm)
error.rate <-(cm[1,2] + cm[2,1]) / sum(cm)
recall <- cm[2,2] / sum(cm[2,])
precision <- cm[2,2] / sum(cm[,2])
cat('Accuracy: ', accuracy)
cat('Error Rate: ', error.rate)
cat('Recall: ', recall)
cat('Precision: ', precision)
}
cm.report(cm1)
cm.report <- function(cm) {
accuracy <- (cm[1,1] + cm[2,2]) / sum(cm)
error.rate <-(cm[1,2] + cm[2,1]) / sum(cm)
recall <- cm[2,2] / sum(cm[2,])
precision <- cm[2,2] / sum(cm[,2])
paste0('Accuracy: ', accuracy, '
Error Rate: ', error.rate, '
Recall: ', recall, '
Precision: ', precision)
}
cm.report(cm1)
cm.report <- function(cm) {
accuracy <- (cm[1,1] + cm[2,2]) / sum(cm)
error.rate <-(cm[1,2] + cm[2,1]) / sum(cm)
recall <- cm[2,2] / sum(cm[2,])
precision <- cm[2,2] / sum(cm[,2])
cat('Accuracy: ', accuracy, '
Error Rate: ', error.rate, '
Recall: ', recall, '
Precision: ', precision)
}
cm.report(cm1)
cm.report(cm2)
rf.cm <- table(rf.preds, test$Private)
tree.cm <- table(tree.preds$Private, test$Private)
cm.report(rf.cm)
cm.report(tree.cm)
library(ISLR)
head(iris)
str(iris)
install.packages('e1071')
library(e1071)
model <- svm(Species ~ ., data=iris)
summary(model)
pred.values <- predict(model, iris[1:4])
table(pred.values)
table(pred.values, iris[,5]])
table(pred.values, iris[,5])
summary(model)
tune.results <- tune(svm, train.x = iris[1:4], train.y = iris[,5], kernel='radial', ranges=list(cost=c(0.1, 1, 10), gamma=c(0.5, 1, 2)))
summary(tune.results)
tune.results <- tune(svm, train.x = iris[1:4], train.y = iris[,5], kernel='radial', ranges=list(cost=c(0.5, 1, 1.5), gamma=c(0.1, 0.5, 0.7)))
summary(tune.results)
tuned.svm <- svm(Species ~ ., data=iris, kernel='radial', cost=1, gamma=0.1)
summary(tuned.svm)
library(tm)
library(twitteR)
library(wordcloud)
library(RColorBrewer)
library(e1017)
library(class)
install.packages('tm')
install.packages('twitteR')
install.packages('wordcloud')
install.packages('e1017')
library(tm)
library(twitteR)
library(wordcloud)
library(e1017)
install.packages('e1017',repos='http://cran.us.r-project.org')
library(ISLR)
View(iris)
library(e1071)
ck <- 'KAxnLeHzVGhHexZzeof4N0h0u'
csk <- 'eV3ff20B5lsD0gWni6lkRodSBVuRqSWRh2BqcHqeKx97hIn2w1'
at <- '850325260905959425-0PQgStKdB5kjHsgPxJ7Hrq88loh2Yse'
ats <- 'UhW18Gau65UULM6kra58WRqWdhFSWDOR52D0yP9g0wuTY'
setup_twitter_oauth(ck, csk, at, ats)
soccer.tweets <- searchTwitter('soccer', n=1000, lang = 'en')
View(soccer.tweets)
soccer.text <- soccer.tweets$getText()
soccer.text <- sapply(soccer.tweets, function(x) x$getText())
soccer.text <- iconv(soccer.text, 'UTF-8', 'ASCII')
soccer.corpus <- Corpus(VectorSource(soccer.text))
term.doc.matrix <- TermDocumentMatrix(soccer.corpus, control = list(
removePunctuation = TRUE
,stopwords = c('soccer', stopwords('english')) # remove words that are very common (e.g. 'the', 'he', 'she', etc... or whatever else).  The stopwords('english') function and parameter remove common english stopwords
,removeNumbers = TRUE
,tolower = TRUE
))
term.doc.matrix <- as.matrix(term.doc.matrix)
word.freq <- sort(rowSums(term.doc.matrix), decreasing = TRUE)
dm <- data.frame(word = names(word.freq), freq = word.freq)
wordcloud(dm$word, dm$freq, random.order = FALSE, colors = brewer.pal(8, 'Dark2'))
soccer.tweets <- searchTwitter('python', n=1000, lang = 'en')
soccer.text <- sapply(soccer.tweets, function(x) x$getText())
soccer.text <- iconv(soccer.text, 'UTF-8', 'ASCII')
soccer.corpus <- Corpus(VectorSource(soccer.text))
term.doc.matrix <- TermDocumentMatrix(soccer.corpus, control = list(
removePunctuation = TRUE
,stopwords = c('http', stopwords('english')) # remove words that are very common (e.g. 'the', 'he', 'she', etc... or whatever else).  The stopwords('english') function and parameter remove common english stopwords
,removeNumbers = TRUE
,tolower = TRUE
))
term.doc.matrix <- as.matrix(term.doc.matrix)
word.freq <- sort(rowSums(term.doc.matrix), decreasing = TRUE)
dm <- data.frame(word = names(word.freq), freq = word.freq)
wordcloud(dm$word, dm$freq, random.order = FALSE, colors = brewer.pal(8, 'Dark2'))
term.doc.matrix <- TermDocumentMatrix(soccer.corpus, control = list(
removePunctuation = TRUE
,stopwords = c('python', 'http', stopwords('english')) # remove words that are very common (e.g. 'the', 'he', 'she', etc... or whatever else).  The stopwords('english') function and parameter remove common english stopwords
,removeNumbers = TRUE
,tolower = TRUE
))
term.doc.matrix <- as.matrix(term.doc.matrix)
word.freq <- sort(rowSums(term.doc.matrix), decreasing = TRUE)
dm <- data.frame(word = names(word.freq), freq = word.freq)
wordcloud(dm$word, dm$freq, random.order = FALSE, colors = brewer.pal(8, 'Dark2'))
term.doc.matrix <- TermDocumentMatrix(soccer.corpus, control = list(
removePunctuation = TRUE
,stopwords = c('python', 'http', 'https', stopwords('english')) # remove words that are very common (e.g. 'the', 'he', 'she', etc... or whatever else).  The stopwords('english') function and parameter remove common english stopwords
,removeNumbers = TRUE
,tolower = TRUE
))
term.doc.matrix <- as.matrix(term.doc.matrix)
word.freq <- sort(rowSums(term.doc.matrix), decreasing = TRUE)
dm <- data.frame(word = names(word.freq), freq = word.freq)
wordcloud(dm$word, dm$freq, random.order = FALSE, colors = brewer.pal(8, 'Dark2'))
library(MASS)
head(Boston)
str(Boston)
View(Boston)
any(is.na(Boston))
any(is.null(Boston))
maxs <- apply(data, margin = 2, max)
maxs <- apply(data, MARGIN = 2, max)
maxs <- apply(data,2,max)
str(data)
data <- Boston
str(data)
maxs <- apply(data, MARGIN = 2, max)
maxs
maxs <- apply(data,2,max)
maxs
mins <- apply(data, MARGIN = 2, min)
mins
scaled.data <- scale(data, center=mins, scale=maxs-mins)
scaled.data <- as.data.frame(scaled.data)
head(scaled.data)
summary(scaled.data)
library(caTools)
set.seed(101)
scaled.data <- scale(data, center=mins, scale=maxs-mins)
scaled <- as.data.frame(scaled.data)
head(scaled)
summary(scaled)
split <- sample.split(scaled$medv, SplitRatio = 0.7)
train <- subset(scaled, split=TRUE)
test <- subset(scaled, split=FALSE)
head(train)
str(train)
str(test)
set.seed(101)
split <- sample.split(scaled$medv, SplitRatio = 0.7)
train <- subset(scaled, split=TRUE)
test <- subset(scaled, split=FALSE)
str(train)
str(test)
str(split)
split==TRUE
sum(split)
length(split)
sum(split)
train <- subset(scaled, split==TRUE)
test <- subset(scaled, split==FALSE)
str(train)
str(test)
library(neuralnet)
install.packages('neuralnet')
library(neuralnet)
n <- names(train)
n
f <- as.formula(paste("medv ~ ", paste(n[!n %in% 'medv'], collapse = ' + ')))
f
nn <- neuralnet(medv ~ .
,data = train
,hidden = c(5,3) # one hidden layer of 5 neurons, a second hidden layer of 3 neurons
,linear.output = TRUE
)
nn <- neuralnet(f # this is the pre-assembled formula, with all the columns being compared against medv
,data = train
,hidden = c(5,3) # one hidden layer of 5 neurons, a second hidden layer of 3 neurons
,linear.output = TRUE
)
plot(nn)
(nn)
plot(nn)
predicted.nn.values <- compute(nn, test)
predicted.nn.values <- compute(nn, test[1:13])
str(predicted.nn.values)
true.predictions <- predicted.nn.values$net.result * (max(data$medv)-min(data$medv))+min(data$medv)
test.r <- (test$medv)*(max(data$medv)-min(data$medv))+min(data$medv)
MSE.nn <- sum((test.r - true.predictions)^2)/nrow(test)
MSE.nn
error.dv <- data.frame(test.r, true.predictions)
error.df <- data.frame(test.r, true.predictions)
head(error.df)
library(ggplot2)
ggplot(error.df, aes(test.r, true.predictions)) + geom_point() + stat_smooth()
setwd("C:/Users/sgenn_000/Documents/GitHub/Udemy/Data Science and Machine Learning Bootcamp with R/Guides/Training Exercises/Machine Learning Projects/CSV files for ML Projects")
head(readLines('bank_note_data.csv'))
df <- read.csv('bank_note_data.csv')
head(df)
str(df)
library(caTools)
set.seed(101)
split <- sample.split(df, SplitRatio = 0.7)
train <- subset(scaled, split==TRUE) # be sure to remember TWO == symbols
test <- subset(scaled, split==FALSE)
str(train)
str(test)
split <- sample.split(df, SplitRatio = 0.7)
train <- subset(df, split==TRUE) # be sure to remember TWO == symbols
test <- subset(df, split==FALSE)
str(train)
str(test)
split <- sample.split(df$Class, SplitRatio = 0.7)
train <- subset(df, split==TRUE) # be sure to remember TWO == symbols
test <- subset(df, split==FALSE)
str(train)
str(test)
library(neuralnet)
n <- names(train)
f <- as.formula(paste("Class ~ ", paste(n[!n %in% 'Class'], collapse = ' + ')))
f # it prints all the non-medv columns in the formula automatically
nn <- neuralnet(f # this is the pre-assembled formula, with all the columns being compared against medv
,data = train
,hidden = c(10) # one hidden layer of 10 neurons
,linear.output = FALSE # this is a CLASSIFICATION problem
)
plot(nn)
ncol(test)
predicted.nn.values <- compute(nn, test[1:ncol(test)-1])
str(predicted.nn.values)
head(predicted.nn.values)
str(train)
str(test)
head(predicted.nn.values$net.result)
predictions <- sapply(predicted.nn.values$net.result, round)
head(predictions)
cm.report <- function(cm) {
accuracy <- (cm[1,1] + cm[2,2]) / sum(cm)
error.rate <-(cm[1,2] + cm[2,1]) / sum(cm)
recall <- cm[2,2] / sum(cm[2,])
precision <- cm[2,2] / sum(cm[,2])
cat('Accuracy: ', accuracy, '
Error Rate: ', error.rate, '
Recall: ', recall, '
Precision: ', precision)
}
cm.report(table(df$Class, predictions))
cm.report(table(test$Class, predictions))
table(test$Class, predictions)
library(randomForest)
df$Class <- factor(df$Class)
set.seed(101)
split = sample.split(df$Class, SplitRatio = 0.70)
train = subset(df, split == TRUE)
test = subset(df, split == FALSE)
str(train)
str(test)
model <- randomForest(Class ~ ., data=train)
rf.pred <- predict(model,test)
table(rf.pred,test$Class)
cm.report(table(test$Class, rf.pred))
plot(nn)
